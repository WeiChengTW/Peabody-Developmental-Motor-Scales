import numpy as np
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

# -------------------------
# å¾æ··æ·†çŸ©é™£è¨ˆç®—æº–ç¢ºç‡
# -------------------------

# ä½ çš„æ··æ·†çŸ©é™£æ•¸æ“š
confusion_matrix_data = np.array([
    [33, 1],   # åœ“å½¢ï¼š33å€‹æ­£ç¢ºï¼Œ1å€‹éŒ¯èª¤
    [4, 81]    # å…¶ä»–ï¼š4å€‹éŒ¯èª¤ï¼Œ81å€‹æ­£ç¢º
])

print("æ··æ·†çŸ©é™£ï¼š")
print(confusion_matrix_data)
print()

# æå–æ•¸å€¼
TP = confusion_matrix_data[0, 0]  # True Positive (åœ“å½¢->åœ“å½¢)
FN = confusion_matrix_data[0, 1]  # False Negative (åœ“å½¢->å…¶ä»–)
FP = confusion_matrix_data[1, 0]  # False Positive (å…¶ä»–->åœ“å½¢)
TN = confusion_matrix_data[1, 1]  # True Negative (å…¶ä»–->å…¶ä»–)

total_samples = TP + TN + FP + FN

print(f"TP (True Positive): {TP}")
print(f"TN (True Negative): {TN}")
print(f"FP (False Positive): {FP}")
print(f"FN (False Negative): {FN}")
print(f"ç¸½æ¨£æœ¬æ•¸: {total_samples}")
print()

# -------------------------
# è¨ˆç®—å„ç¨®è©•ä¼°æŒ‡æ¨™
# -------------------------

# 1. æ•´é«”æº–ç¢ºç‡ (Overall Accuracy)
accuracy = (TP + TN) / total_samples
print(f"æ•´é«”æº–ç¢ºç‡ (Overall Accuracy): {accuracy:.4f} ({accuracy*100:.2f}%)")

# 2. åœ“å½¢é¡åˆ¥çš„ç²¾ç¢ºç‡ (Precision for Circle)
precision_circle = TP / (TP + FP) if (TP + FP) > 0 else 0
print(f"åœ“å½¢ç²¾ç¢ºç‡ (Circle Precision): {precision_circle:.4f} ({precision_circle*100:.2f}%)")

# 3. åœ“å½¢é¡åˆ¥çš„å¬å›ç‡ (Recall for Circle)
recall_circle = TP / (TP + FN) if (TP + FN) > 0 else 0
print(f"åœ“å½¢å¬å›ç‡ (Circle Recall): {recall_circle:.4f} ({recall_circle*100:.2f}%)")

# 4. åœ“å½¢é¡åˆ¥çš„F1åˆ†æ•¸
f1_circle = 2 * (precision_circle * recall_circle) / (precision_circle + recall_circle) if (precision_circle + recall_circle) > 0 else 0
print(f"åœ“å½¢F1åˆ†æ•¸ (Circle F1-Score): {f1_circle:.4f} ({f1_circle*100:.2f}%)")

print("\n" + "="*50)

# 5. å…¶ä»–é¡åˆ¥çš„ç²¾ç¢ºç‡
precision_other = TN / (TN + FN) if (TN + FN) > 0 else 0
print(f"å…¶ä»–ç²¾ç¢ºç‡ (Other Precision): {precision_other:.4f} ({precision_other*100:.2f}%)")

# 6. å…¶ä»–é¡åˆ¥çš„å¬å›ç‡
recall_other = TN / (TN + FP) if (TN + FP) > 0 else 0
print(f"å…¶ä»–å¬å›ç‡ (Other Recall): {recall_other:.4f} ({recall_other*100:.2f}%)")

# 7. å…¶ä»–é¡åˆ¥çš„F1åˆ†æ•¸
f1_other = 2 * (precision_other * recall_other) / (precision_other + recall_other) if (precision_other + recall_other) > 0 else 0
print(f"å…¶ä»–F1åˆ†æ•¸ (Other F1-Score): {f1_other:.4f} ({f1_other*100:.2f}%)")

# -------------------------
# ç¹ªè£½æ··æ·†çŸ©é™£ç†±åŠ›åœ–
# -------------------------
def plot_confusion_matrix(cm, class_names=['Circle', 'Other']):
    """ç¹ªè£½æ··æ·†çŸ©é™£ç†±åŠ›åœ–"""
    plt.figure(figsize=(8, 6))
    
    # è¨ˆç®—ç™¾åˆ†æ¯”
    cm_percent = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100
    
    # å‰µå»ºæ¨™ç±¤ï¼ˆæ•¸é‡ + ç™¾åˆ†æ¯”ï¼‰
    labels = np.array([[f'{cm[i,j]}\n({cm_percent[i,j]:.1f}%)' 
                       for j in range(cm.shape[1])] 
                       for i in range(cm.shape[0])])
    
    # ç¹ªè£½ç†±åŠ›åœ–
    sns.heatmap(cm, annot=labels, fmt='', cmap='Blues', 
                xticklabels=class_names, yticklabels=class_names,
                cbar_kws={'label': 'Count'})
    
    plt.title(f'Confusion Matrix\nOverall Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)', 
              fontsize=14, fontweight='bold')
    plt.ylabel('True Label', fontsize=12)
    plt.xlabel('Predicted Label', fontsize=12)
    
    # æ·»åŠ çµ±è¨ˆä¿¡æ¯
    plt.figtext(0.02, 0.02, 
                f'Total Samples: {total_samples}\n'
                f'Correct Predictions: {TP + TN}\n'
                f'Wrong Predictions: {FP + FN}',
                fontsize=10, bbox=dict(boxstyle="round,pad=0.3", facecolor="lightgray"))
    
    plt.tight_layout()
    plt.show()

plot_confusion_matrix(confusion_matrix_data)

# -------------------------
# æ¨¡å‹æ€§èƒ½è©•ä¼°å ±å‘Š
# -------------------------
print("\n" + "="*60)
print("æ¨¡å‹æ€§èƒ½è©•ä¼°å ±å‘Š")
print("="*60)

print(f"æ•¸æ“šé›†å¤§å°: {total_samples} å¼µåœ–ç‰‡")
print(f"åœ“å½¢åœ–ç‰‡: {TP + FN} å¼µ")
print(f"å…¶ä»–åœ–ç‰‡: {TN + FP} å¼µ")
print()

print("åˆ†é¡çµæœ:")
print(f"â€¢ æ­£ç¢ºé æ¸¬: {TP + TN} å¼µ ({((TP + TN)/total_samples)*100:.2f}%)")
print(f"â€¢ éŒ¯èª¤é æ¸¬: {FP + FN} å¼µ ({((FP + FN)/total_samples)*100:.2f}%)")
print()

print("éŒ¯èª¤åˆ†æ:")
print(f"â€¢ åœ“å½¢è¢«èª¤åˆ¤ç‚ºå…¶ä»–: {FN} å¼µ")
print(f"â€¢ å…¶ä»–è¢«èª¤åˆ¤ç‚ºåœ“å½¢: {FP} å¼µ")
print()

# åˆ¤æ–·æ¨¡å‹æ€§èƒ½
if accuracy >= 0.95:
    performance_level = "å„ªç§€"
    color_code = "ğŸŸ¢"
elif accuracy >= 0.90:
    performance_level = "è‰¯å¥½"
    color_code = "ğŸŸ¡"
elif accuracy >= 0.80:
    performance_level = "ä¸€èˆ¬"
    color_code = "ğŸŸ "
else:
    performance_level = "éœ€è¦æ”¹é€²"
    color_code = "ğŸ”´"

print(f"æ¨¡å‹æ€§èƒ½ç­‰ç´š: {color_code} {performance_level}")
print(f"æ•´é«”æº–ç¢ºç‡: {accuracy:.4f} ({accuracy*100:.2f}%)")

# -------------------------
# ä½¿ç”¨æ¨¡å‹é€²è¡Œé æ¸¬çš„ç¯„ä¾‹ç¨‹å¼ç¢¼
# -------------------------
print("\n" + "="*60)
print("åœ¨å¯¦éš›ä½¿ç”¨ä¸­è¨ˆç®—æº–ç¢ºç‡çš„ç¨‹å¼ç¢¼ç¯„ä¾‹:")
print("="*60)

example_code = '''
# è¼‰å…¥è¨“ç·´å¥½çš„æ¨¡å‹
model = tf.keras.models.load_model('circle_or_oval_resnet50.h5')

# åœ¨æ¸¬è©¦é›†ä¸Šé€²è¡Œé æ¸¬
predictions = model.predict(test_generator)
predicted_classes = np.argmax(predictions, axis=1)

# ç²å–çœŸå¯¦æ¨™ç±¤
true_classes = test_generator.classes

# è¨ˆç®—æº–ç¢ºç‡
from sklearn.metrics import accuracy_score, classification_report
accuracy = accuracy_score(true_classes, predicted_classes)
print(f"æ¸¬è©¦é›†æº–ç¢ºç‡: {accuracy:.4f} ({accuracy*100:.2f}%)")

# è©³ç´°åˆ†é¡å ±å‘Š
report = classification_report(true_classes, predicted_classes)
print("åˆ†é¡å ±å‘Š:")
print(report)
'''

print(example_code)